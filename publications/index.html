<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width initial-scale=1" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
      
        <title>Robotics Group @ University of Montreal | Publications</title>
        <meta name="description" content="The Robotics and Embodied AI Lab @ U de Montreal">
      
        <link rel="shortcut icon" href="https://montrealrobotics.github.io/assets/img/favicon.png">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css"
              integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
              crossorigin="anonymous">
        <link rel="stylesheet" href="https://montrealrobotics.github.io/assets/css/sb-admin-2.min.css">
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
        <link rel="stylesheet" href="https://montrealrobotics.github.io/assets/css/main.css">
        <link rel="canonical" href="https://montrealrobotics.github.io/publications/">

      </head>
    <body>
        <div class="container">
            <!-- This is a bit nasty, but it basically says be a column first, and on larger screens be a spaced out row -->
            <nav class="navbar sticky-top navbar-expand-lg navbar-light navbar-dark bg-primary mb-2 text-white">
                <a class="navbar-brand" href="/">REAL</a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                      </button>
                      <div class="collapse navbar-collapse" id="navbarNav">
                        <ul class="navbar-nav">
                                
                                
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/">
                                            Home
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/people.html">
                                            People
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/research.html">
                                            Research
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link active"
                                           href="/publications/">
                                            Publications
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/contact.html">
                                            Contact
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="">
                                            Blog
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/blog.html">
                                            
                                        </a>
                                    </li>
                                    
                        </ul>
                      </div>
            </nav>

            

            <!-- 
                <h1>Publications</h1>
             -->

            <div class="card border-bottom-primary shadow py-2 mb-4">
        <div class="card-body">
          <div class="row no-gutters align-items-center">
            <div class="col mr-2">
              <div class="h2 font-weight-bold text-primary mb-1">Publications</div>
            </div>
          </div>
        </div>
</div>

<div class="card border-left-primary shadow mb-1">
          <div class="card-body">
            <div class="h2 font-weight-bold text-primary mb-1">2020</div>
          </div>
  </div>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="gradslam">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/gradslam.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>gradSLAM: Dense SLAM meets automatic differentiation</b></span>
    <span class="author">
      
      	
        
          
            
              Jatavallabhula Krishna Murthy,
            
          
        
      
      	
        
          
            
              Ganesh Iyer,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Robotics and Automation (ICRA)</em>
    
    
      2020
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1910.10672" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
    <a href="https://gradslam.github.io" target="_blank" class="buttonPP">Project Page</a>
  
  
    <a href="https://github.com/gradslam/gradslam" target="_blank" class="buttonPP">Code</a>
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>The question of "representation" is central in the context of dense simultaneous localization and mapping (SLAM). Newer learning-based approaches have the potential to leverage data or task performance to directly inform the choice of representation. However, learning representations for SLAM has been an open question, because traditional SLAM systems are not end-to-end differentiable.In this work, we present gradSLAM, a differentiable computational graph take on SLAM. Leveraging the automatic differentiation capabilities of computational graphs, gradSLAM enables the design of SLAM systems that allow for gradient-based learning across each of their components, or the system as a whole. This is achieved by creating differentiable alternatives for each non-differentiable component in a typical dense SLAM system. Specifically, we demonstrate how to design differentiable trust-region optimizers, surface measurement and fusion schemes, as well as differentiate over rays, without sacrificing performance. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ gradslam, <br />
      author = {  
                  
                      Krishna Murthy, Jatavallabhula and
                    
                   
                  
                      Iyer, Ganesh and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { gradSLAM: Dense SLAM meets automatic differentiation }, <br />
      
      
        booktitle = { International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2020 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="lamaml">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/lamaml.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>La-MAML: Look-ahead Meta Learning for Continual Learning</b></span>
    <span class="author">
      
      	
        
          
            
              Gunshi Gupta,
            
          
        
      
      	
        
          
            
              Karmesh Yadav,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Neural Information Processing Systems (Neurips)</em>
    
    
      2020
    
    
      <b> Oral (top 1.1%) </b>
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/2007.13904" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
    <a href="https://mila.quebec/en/article/la-maml-look-ahead-meta-learning-for-continual-learning/" target="_blank" class="buttonPP">Project Page</a>
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>The continual learning problem involves training models with limited capacity to perform well on a set of an unknown number of sequentially arriving tasks. While meta-learning shows great potential for reducing interference between old and new tasks, the current training procedures tend to be either slow or offline, and sensitive to many hyper-parameters. In this work, we propose Look-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm for online-continual learning, aided by a small episodic memory. Our proposed modulation of per-parameter learning rates in our meta-learning update allows us to draw connections to prior work on hypergradients and meta-descent. This provides a more flexible and efficient way to mitigate catastrophic forgetting compared to conventional prior-based methods. La-MAML achieves performance superior to other replay-based, prior-based and meta-learning based approaches for continual learning on real-world visual classification benchmarks.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ lamaml, <br />
      author = {  
                  
                      Gupta, Gunshi and
                    
                   
                  
                      Yadav, Karmesh and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { La-MAML: Look-ahead Meta Learning for Continual Learning }, <br />
      
      
        booktitle = { Neural Information Processing Systems (Neurips) }, <br />
      
      
        year = { 2020 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="che2020neurips">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Your GAN is Secretly an Energy-based Model and You Should Use Discriminator Driven Latent Sampling</b></span>
    <span class="author">
      
      	
        
          
            
              Tong Che,
            
          
        
      
      	
        
          
            
              Ruixiang Zhang,
            
          
        
      
      	
        
          
            
              Jascha Sohl-Dickstein,
            
          
        
      
      	
        
          
            
              Hugo Larochelle,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              Yuan Cao,
            
          
        
      
      	
        
          and
          
            
              Yoshua Bengio
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Neural Information Processing Systems (Neurips)</em>
    
    
      2020
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/2003.06060" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>We show that the sum of the implicit generator log-density of a GAN with the logit score of the discriminator defines an energy function which yields the true data density when the generator is imperfect but the discriminator is optimal, thus making it possible to improve on the typical generator. To make that practical, we show that sampling from this modified density can be achieved by sampling in latent space according to an energy-based model induced by the sum of the latent prior log-density and the discriminator output score. This can be achieved by running a Langevin MCMC in latent space and then applying the generator function, which we call Discriminator Driven Latent Sampling (DDLS). We show that DDLS is highly efficient compared to previous methods which work in the high-dimensional pixel space and can be applied to improve on previously trained GANs of many types. We evaluate DDLS on both synthetic and real-world datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially improves the Inception Score of an off-the-shelf pre-trained SN-GAN from 8.22 to 9.09 which is even comparable to the class-conditional BigGAN model. This achieves a new state-of-the-art in unconditional image synthesis setting without introducing extra parameters or additional training.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ che2020neurips, <br />
      author = {  
                  
                      Che, Tong and
                    
                   
                  
                      Zhang, Ruixiang and
                    
                   
                  
                      Sohl-Dickstein, Jascha and
                    
                   
                  
                      Larochelle, Hugo and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Cao, Yuan and
                    
                   
                  
                      Bengio, Yoshua 
                    
                   }, <br />
      title = { Your GAN is Secretly an Energy-based Model and You Should Use Discriminator Driven Latent Sampling }, <br />
      
      
        booktitle = { Neural Information Processing Systems (Neurips) }, <br />
      
      
        year = { 2020 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="mehta2020curriculum">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/mehta2020curriculum.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Curriculum in Gradient-Based Meta-Reinforcement Learning</b></span>
    <span class="author">
      
      	
        
          
            
              Bhairav Mehta,
            
          
        
      
      	
        
          
            
              Tristan Deleu,
            
          
        
      
      	
        
          
            
              Sharath Chandra Raparthy,
            
          
        
      
      	
        
          
            
              Christopher Pal,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In BETR-RL Workshop</em>
    
    
      2020
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/2002.07956" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Can Meta-RL use curriculum learning? In this work, we explore that question and find that curriculum learning stabilizes meta-RL in complex navigation and locomotion tasks. We also highlight issues with Meta-RL benchmarks by highlighting failure cases when we vary task distributions.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ mehta2020curriculum, <br />
      author = {  
                  
                      Mehta, Bhairav and
                    
                   
                  
                      Deleu, Tristan and
                    
                   
                  
                      Raparthy, Sharath Chandra and
                    
                   
                  
                      Pal, Christopher and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Curriculum in Gradient-Based Meta-Reinforcement Learning }, <br />
      
      
        booktitle = { BETR-RL Workshop }, <br />
      
      
        year = { 2020 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="ssadr">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/ssadr.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Generating Automatic Curricula via Self-Supervised Active Domain Randomization</b></span>
    <span class="author">
      
      	
        
          
            
              Sharath Chandra Raparthy,
            
          
        
      
      	
        
          
            
              Bhairav Mehta,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In BETR-RL Workshop</em>
    
    
      2020
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/2002.07911" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
    <a href="https://github.com/montrealrobotics/unsupervised-adr" target="_blank" class="buttonPP">Code</a>
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Can you learn domain randomization curricula with no rewards? We show that agents trained via self-play in the ADR framework outperform uniform domain randomization by magnitudes in both simulated and real-world transfer.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ ssadr, <br />
      author = {  
                  
                      Raparthy, Sharath Chandra and
                    
                   
                  
                      Mehta, Bhairav and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Generating Automatic Curricula via Self-Supervised Active Domain Randomization }, <br />
      
      
        booktitle = { BETR-RL Workshop }, <br />
      
      
        year = { 2020 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="aido2018">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/aido18.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>The AI Driving Olympics at NeurIPS 2018</b></span>
    <span class="author">
      
      	
        
          
            
              Julian Zilly,
            
          
        
      
      	
        
          
            
              Jacopo Tani,
            
          
        
      
      	
        
          
            
              Breandan Considine,
            
          
        
      
      	
        
          
            
              Bhairav Mehta,
            
          
        
      
      	
        
          
            
              Andrea F Daniele,
            
          
        
      
      	
        
          
            
              Manfred Diaz,
            
          
        
      
      	
        
          
            
              Gianmarco Bernasconi,
            
          
        
      
      	
        
          
            
              Jan Ruch,
            
          
        
      
      	
        
          
            
              Florian Golemo,
            
          
        
      
      	
        
          
            
              A Kirsten Bowser,
            
          
        
      
      	
        
          
            
              Matthew R Walter,
            
          
        
      
      	
        
          
            
              Ruslan Hristov,
            
          
        
      
      	
        
          
            
              Sunil Mallya,
            
          
        
      
      	
        
          
            
              Emilio Frazzoli,
            
          
        
      
      	
        
          
            
              Andrea Censi,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Springer</em>
    
    
      2020
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1903.02503" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Despite recent breakthroughs, the ability of deep learning and reinforcement learning to outperform traditional approaches to control physically embodied robotic agents remains largely unproven. To help bridge this gap, we created the “AI Driving Olympics” (AI-DO), a competition with the objective of evaluating the state of the art in machine learning and artificial intelligence for mobile robotics. Based on the simple and well specified autonomous driving and navigation environment called “Duckietown,” AI-DO includes a series of tasks of increasing complexity – from simple lane-following to fleet management. For each task, we provide tools for competitors to use in the form of simulators, logs, code templates, baseline implementations and low-cost access to robotic hardware. We evaluate submissions in simulation online, on standardized hardware environments, and finally at the competition event. The first AI-DO, AI-DO 1, occurred at the Neural Information Processing Systems (NeurIPS) conference in December 2018. The results of AI-DO 1 highlight the need for better benchmarks, which are lacking in robotics, as well as improved mechanisms to bridge the gap between simulation and reality.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ aido2018, <br />
      author = {  
                  
                      Zilly, Julian and
                    
                   
                  
                      Tani, Jacopo and
                    
                   
                  
                      Considine, Breandan and
                    
                   
                  
                      Mehta, Bhairav and
                    
                   
                  
                      Daniele, Andrea F and
                    
                   
                  
                      Diaz, Manfred and
                    
                   
                  
                      Bernasconi, Gianmarco and
                    
                   
                  
                      Ruch, Jan and
                    
                   
                  
                      Golemo, Florian and
                    
                   
                  
                      Bowser, A Kirsten and
                    
                   
                  
                      Walter, Matthew R and
                    
                   
                  
                      Hristov, Ruslan and
                    
                   
                  
                      Mallya, Sunil and
                    
                   
                  
                      Frazzoli, Emilio and
                    
                   
                  
                      Censi, Andrea and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { The AI Driving Olympics at NeurIPS 2018 }, <br />
      
      
        booktitle = { Springer }, <br />
      
      
        year = { 2020 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="probod">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/probod.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Probabilistic Object Detection: Strenghts, Weaknesses, and Opportunities</b></span>
    <span class="author">
      
      	
        
          
            
              Dhaivat Bhatt,
            
          
        
      
      	
        
          
            
              Dishank Bansal,
            
          
        
      
      	
        
          
            
              Gunshi Gupta,
            
          
        
      
      	
        
          
            
              Krishna Murthy Jatavallabhula,
            
          
        
      
      	
        
          
            
              Hanju Lee,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In ICML workshop on AI for autonomous driving</em>
    
    
      2020
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
  
  
  
  
  
  
    <a href="https://gunshigupta.netlify.app/publication/probod/" target="_blank" class="buttonPP">Project Page</a>
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Deep neural networks are the de-facto standard for object detection in autonomous driving applications. However, neural networks cannot be blindly trusted even within the training data distribution, let alone outside it. This has paved way for several probabilistic object detection techniques that measure uncertainty in the outputs of an object detector. Through this position paper, we serve three main purposes. First, we briefly sketch the landscape of current methods for probabilistic object detection. Second, we present the main shortcomings of these approaches. Finally, we present promising avenues for future research, and proof-of-concept results where applicable. Through this effort, we hope to bring the community one step closer to performing accurate, reliable, and consistent probabilistic object detection.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ probod, <br />
      author = {  
                  
                      Bhatt, Dhaivat and
                    
                   
                  
                      Bansal, Dishank and
                    
                   
                  
                      Gupta, Gunshi and
                    
                   
                  
                      Jatavallabhula, Krishna Murthy and
                    
                   
                  
                      Lee, Hanju and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Probabilistic Object Detection: Strenghts, Weaknesses, and Opportunities }, <br />
      
      
        booktitle = { ICML workshop on AI for autonomous driving }, <br />
      
      
        year = { 2020 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<div class="card border-left-primary shadow mb-1">
          <div class="card-body">
            <div class="h2 font-weight-bold text-primary mb-1">2019</div>
          </div>
  </div>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="bharadhwaj2018data">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/homanga2019icra.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies</b></span>
    <span class="author">
      
      	
        
          
            
              Homanga Bharadhwaj,
            
          
        
      
      	
        
          
            
              Zihan Wang,
            
          
        
      
      	
        
          
            
              Yoshua Bengio,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2019
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1810.04871" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage \textitsimulation and \textitoff-policy data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is trained through a meta-learning strategy in simulation first. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with far fewer real world expert demonstrations, we show successful planning performances in different navigation tasks.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ bharadhwaj2018data, <br />
      author = {  
                  
                      Bharadhwaj, Homanga and
                    
                   
                  
                      Wang, Zihan and
                    
                   
                  
                      Bengio, Yoshua and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies }, <br />
      
      
        booktitle = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2019 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="sai2019dal">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/dal.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Deep Active Localization</b></span>
    <span class="author">
      
      	
        
          
            
              Sai Krishna,
            
          
        
      
      	
        
          
            
              Keehong Seo,
            
          
        
      
      	
        
          
            
              Dhaivat Bhatt,
            
          
        
      
      	
        
          
            
              Vincent Mai,
            
          
        
      
      	
        
          
            
              Krishna Murthy,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Robotics and Automation Letters (RAL)</em>
    
    
      2019
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1903.01669" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
    <a href="https://github.com/montrealrobotics/dal" target="_blank" class="buttonPP">Code</a>
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Active localization is the problem of generating robot actions that allow it to maximally disambiguate its pose within a reference map. Traditional approaches to this use an information-theoretic criterion for action selection and hand-crafted perceptual models. In this work we propose an end-to-end differentiable method for learning to take informative actions that is trainable entirely in simulation and then transferable to real robot hardware with zero refinement. The system is composed of two modules: a convolutional neural network for perception, and a deep reinforcement learned planning module. We introduce a multi-scale approach to the learned perceptual model since the accuracy needed to perform action selection with reinforcement learning is much less than the accuracy needed for robot control. We demonstrate that the resulting system outperforms using the traditional approach for either perception or planning. We also demonstrate our approaches robustness to different map configurations and other nuisance parameters through the use of domain randomization in training. The code is also compatible with the OpenAI gym framework, as well as the Gazebo simulator.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ sai2019dal, <br />
      author = {  
                  
                      Krishna, Sai and
                    
                   
                  
                      Seo, Keehong and
                    
                   
                  
                      Bhatt, Dhaivat and
                    
                   
                  
                      Mai, Vincent and
                    
                   
                  
                      Murthy, Krishna and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Deep Active Localization }, <br />
      
      
        booktitle = { IEEE Robotics and Automation Letters (RAL) }, <br />
      
      
        year = { 2019 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="adr">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/adr.gif" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Active Domain Randomization</b></span>
    <span class="author">
      
      	
        
          
            
              Bhairav Mehta,
            
          
        
      
      	
        
          
            
              Manfred Diaz,
            
          
        
      
      	
        
          
            
              Florian Golemo,
            
          
        
      
      	
        
          
            
              Christopher Pal,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Conference on Robot Learning (CoRL)</em>
    
    
      2019
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1904.04762" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
    <a href="https://github.com/montrealrobotics/active-domainrand" target="_blank" class="buttonPP">Code</a>
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>We tackle the uniform sampling assumption in domain randomization and learn a randomization strategy, looking for the most informative environments. Our method shows significant improvements in agent performance, agent generalization, sample complexity, and interpretability over the traditional domain and dynamics randomization strategies.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ adr, <br />
      author = {  
                  
                      Mehta, Bhairav and
                    
                   
                  
                      Diaz, Manfred and
                    
                   
                  
                      Golemo, Florian and
                    
                   
                  
                      Pal, Christopher and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Active Domain Randomization }, <br />
      
      
        booktitle = { Conference on Robot Learning (CoRL) }, <br />
      
      
        year = { 2019 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<div class="card border-left-primary shadow mb-1">
          <div class="card-body">
            <div class="h2 font-weight-bold text-primary mb-1">2018</div>
          </div>
  </div>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="paull2018probabilistic">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/paull2018coverage.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Probabilistic cooperative mobile robot area coverage and its application to autonomous seabed mapping</b></span>
    <span class="author">
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              Mae Seto,
            
          
        
      
      	
        
          
            
              John J Leonard,
            
          
        
      
      	
        
          and
          
            
              Howard Li
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In </em>
    
    
      2018
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>There are many applications that require mobile robots to autonomously cover an entire area with a sensor or end effector. The vast majority of the literature on this subject is focused on addressing path planning for area coverage under the assumption that the robot’s pose is known or that error is bounded. In this work, we remove this assumption and develop a completely probabilistic representation of coverage. We show that coverage is guaranteed as long as the robot pose estimates are consistent, a much milder assumption than zero or bounded error. After formally connecting robot sensor uncertainty with area coverage, we propose an adaptive sliding window filter pose estimator that provides a close approximation to the full maximum a posteriori estimate with a computation cost that is bounded over time. Subsequently, an adaptive planning strategy is presented that automatically exploits conditions of low vehicle uncertainty to more efficiently cover an area. We further extend this approach to the multi-robot case where robots can communicate through a (possibly faulty and low-bandwidth) channel and make relative measurements of one another. In this case, area coverage is achieved more quickly since the uncertainty over the robots’ trajectories is reduced. We apply the framework to the scenario of mapping an area of seabed with an autonomous underwater vehicle. Experimental results support the claim that our method achieves guaranteed complete coverage notwithstanding poor navigational sensors and that resulting path lengths required to cover the entire area are shortest using the proposed cooperative and adaptive approach.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ paull2018probabilistic, <br />
      author = {  
                  
                      Paull, Liam and
                    
                   
                  
                      Seto, Mae and
                    
                   
                  
                      Leonard, John J and
                    
                   
                  
                      Li, Howard 
                    
                   }, <br />
      title = { Probabilistic cooperative mobile robot area coverage and its application to autonomous seabed mapping }, <br />
      
        journal = { The International Journal of Robotics Research }, <br />
      
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="ort2018icra">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/ort2018icra.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Autonomous Vehicle Navigation in Rural Environments without Detailed Prior Maps</b></span>
    <span class="author">
      
      	
        
          
            
              Teddy Ort,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          and
          
            
              Daniela Rus
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2018
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/https://toyota.csail.mit.edu/sites/default/files/documents/papers/ICRA2018_AutonomousVehicleNavigationRuralEnvironment.pdf" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>State-of-the-art autonomous driving systems rely heavily on detailed and highly accurate prior maps. However, outside of small urban areas, it is very challenging to build, store, and transmit detailed maps since the spatial scales are so large. Furthermore, maintaining detailed maps of large rural areas can be impracticable due to the rapid rate at which these environments can change. This is a significant limitation for the widespread applicability of autonomous driving technology, which has the potential for an incredibly positive societal impact. In this paper, we address the problem of autonomous navigation in rural environments through a novel mapless driving framework that combines sparse topological maps for global navigation with a sensor-based perception system for local navigation. First, a local navigation goal within the sensor view of the vehicle is chosen as a waypoint leading towards the global goal. Next, the local perception system generates a feasible trajectory in the vehicle frame to reach the waypoint while abiding by the rules of the road for the segment being traversed. These trajectories are updated to remain in the local frame using the vehicle’s odometry and the associated uncertainty based on the least-squares residual and a recursive filtering approach, which allows the vehicle to navigate road networks reliably, and at high speed, without detailed prior maps. We demonstrate the performance of the system on a full-scale autonomous vehicle navigating in a challenging rural environment and benchmark the system on a large amount of collected data.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ ort2018icra, <br />
      author = {  
                  
                      Ort, Teddy and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Rus, Daniela 
                    
                   }, <br />
      title = { Autonomous Vehicle Navigation in Rural Environments without Detailed Prior Maps }, <br />
      
      
        booktitle = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="mai2018local">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/mai2018blimp.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Local Positioning System Using UWB Range Measurements for an Unmanned Blimp</b></span>
    <span class="author">
      
      	
        
          
            
              Vincent Mai,
            
          
        
      
      	
        
          
            
              Mina Kamel,
            
          
        
      
      	
        
          
            
              Matthias Krebs,
            
          
        
      
      	
        
          
            
              Andreas Schaffner,
            
          
        
      
      	
        
          
            
              Daniel Meier,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          and
          
            
              Roland Siegwart
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In </em>
    
    
      2018
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/https://ieeexplore.ieee.org/document/8392389" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Unmanned blimps are a safe and reliable alternative to conventional drones when flying above people. On-board real-time tracking of their pose and velocities is a necessary step toward autonomous navigation. There is a need for an easily deployable technology that is able to accurately and robustly estimate the pose and velocities of a blimp in 6 DOF, as well as unexpected applied forces and torques, in an uncontrolled environment. We present two multiplicative extended Kalman filters using ultrawideband radio sensors and a gyroscope to address this challenge. One filter is updated using a dynamics model of the blimp, whereas the other uses a constant speed model. We describe a set of experiments in which these estimators have been implemented on an embedded flight controller. They were tested and compared in accuracy and robustness in a hardware-in-loop simulation as well as on a real blimp. This approach can be generalized to any lighter than air robot to track it with the necessary accuracy, precision, and robustness to allow autonomous navigation.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ mai2018local, <br />
      author = {  
                  
                      Mai, Vincent and
                    
                   
                  
                      Kamel, Mina and
                    
                   
                  
                      Krebs, Matthias and
                    
                   
                  
                      Schaffner, Andreas and
                    
                   
                  
                      Meier, Daniel and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Siegwart, Roland 
                    
                   }, <br />
      title = { Local Positioning System Using UWB Range Measurements for an Unmanned Blimp }, <br />
      
        journal = { IEEE Robotics and Automation Letters }, <br />
      
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="CTCNet">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/ctcnet.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Geometric Consistency for Self-Supervised End-to-End Visual Odometry</b></span>
    <span class="author">
      
      	
        
          
            
              Ganesh Iyer,
            
          
        
      
      	
        
          
            
              J Krishna Murthy,
            
          
        
      
      	
        
          
            
              K Gunshi Gupta,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In CVPR Workshop on Deep Learning for Visual SLAM</em>
    
    
      2018
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1804.03789" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
    <a href="https://krrish94.github.io/CTCNet-release/" target="_blank" class="buttonPP">Project Page</a>
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose "Composite Transformation Constraints (CTCs)", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ CTCNet, <br />
      author = {  
                  
                      Iyer, Ganesh and
                    
                   
                  
                      Murthy, J Krishna and
                    
                   
                  
                      Gunshi Gupta, K and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Geometric Consistency for Self-Supervised End-to-End Visual Odometry }, <br />
      
      
        booktitle = { CVPR Workshop on Deep Learning for Visual SLAM }, <br />
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="amini2018learning">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/amini2018parallel.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Learning steering bounds for parallel autonomous systems</b></span>
    <span class="author">
      
      	
        
          
            
              Alexander Amini,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              Thomas Balch,
            
          
        
      
      	
        
          
            
              Sertac Karaman,
            
          
        
      
      	
        
          and
          
            
              Daniela Rus
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2018
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/https://dspace.mit.edu/handle/1721.1/117632" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Deep learning has been successfully applied to “end-to-end” learning of the autonomous driving task, where a deep neural network learns to predict steering control commands from camera data input. While these previous works support reactionary control, the representation learned is not usable for higher-level decision making required for autonomous navigation. This paper tackles the problem of learning a representation to predict a continuous control probability distribution, and thus steering control options and bounds for those options, which can be used for autonomous navigation. Each mode of the distribution encodes a possible macro-action that the system could execute at that instant, and the covariances of the modes place bounds on safe steering control values. Our approach has the added advantage of being trained on unlabeled data collected from inexpensive cameras. The deep neural network based algorithm generates a probability distribution over the space of steering angles, from which we leverage Variational Bayesian methods to extract a mixture model and compute the different possible actions in the environment. A bound, which the autonomous vehicle must respect in our parallel autonomy setting, is then computed for each of these actions. We evaluate our approach on a challenging dataset containing a wide variety of driving conditions, and show that our algorithm is capable of parameterizing Gaussian Mixture Models for possible actions, and extract steering bounds with a mean error of only 2 degrees. Additionally, we demonstrate our system working on a full scale autonomous vehicle and evaluate its ability to successful handle various different parallel autonomy situations.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ amini2018learning, <br />
      author = {  
                  
                      Amini, Alexander and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Balch, Thomas and
                    
                   
                  
                      Karaman, Sertac and
                    
                   
                  
                      Rus, Daniela 
                    
                   }, <br />
      title = { Learning steering bounds for parallel autonomous systems }, <br />
      
      
        booktitle = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<div class="card border-left-primary shadow mb-1">
          <div class="card-body">
            <div class="h2 font-weight-bold text-primary mb-1">2017</div>
          </div>
  </div>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="paull2017duckietown">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/paull2017duckietown.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Duckietown: an open, inexpensive and flexible platform for autonomy education and research</b></span>
    <span class="author">
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              Jacopo Tani,
            
          
        
      
      	
        
          
            
              Heejin Ahn,
            
          
        
      
      	
        
          
            
              Javier Alonso-Mora,
            
          
        
      
      	
        
          
            
              Luca Carlone,
            
          
        
      
      	
        
          
            
              Michal Cap,
            
          
        
      
      	
        
          
            
              Yu Fan Chen,
            
          
        
      
      	
        
          
            
              Changhyun Choi,
            
          
        
      
      	
        
          
            
              Jeff Dusek,
            
          
        
      
      	
        
          
            
              Yajun Fang,
            
          
        
      
      	
        
          and
          
            
               others
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2017
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/http://www.mit.edu/ hangzhao/papers/duckietown.pdf" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Duckietown is an open, inexpensive and flexible platform for autonomy education and research. The platform comprises small autonomous vehicles (“Duckiebots”) built from off-the-shelf components, and cities (“Duckietowns”) complete with roads, signage, traffic lights, obstacles, and citizens (duckies) in need of transportation. The Duckietown platform offers a wide range of functionalities at a low cost. Duckiebots sense the world with only one monocular camera and perform all processing onboard with a Raspberry Pi 2, yet are able to: follow lanes while avoiding obstacles, pedestrians (duckies) and other Duckiebots, localize within a global map, navigate a city, and coordinate with other Duckiebots to avoid collisions. Duckietown is a useful tool since educators and researchers can save money and time by not having to develop all of the necessary supporting infrastructure and capabilities. All materials are available as open source, and the hope is that others in the community will adopt the platform for education and research.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ paull2017duckietown, <br />
      author = {  
                  
                      Paull, Liam and
                    
                   
                  
                      Tani, Jacopo and
                    
                   
                  
                      Ahn, Heejin and
                    
                   
                  
                      Alonso-Mora, Javier and
                    
                   
                  
                      Carlone, Luca and
                    
                   
                  
                      Cap, Michal and
                    
                   
                  
                      Chen, Yu Fan and
                    
                   
                  
                      Choi, Changhyun and
                    
                   
                  
                      Dusek, Jeff and
                    
                   
                  
                      Fang, Yajun and
                    
                   
                  
                      others,  
                    
                   }, <br />
      title = { Duckietown: an open, inexpensive and flexible platform for autonomy education and research }, <br />
      
      
        booktitle = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2017 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="rosman2017hybrid">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/rosman2017hybrid.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Hybrid control and learning with coresets for autonomous vehicles</b></span>
    <span class="author">
      
      	
        
          
            
              Guy Rosman,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          and
          
            
              Daniela Rus
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
    
    
      2017
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/http://people.csail.mit.edu/rosman/papers/ctrl_embedding.pdf" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Modern autonomous systems such as driverless vehicles need to safely operate in a wide range of conditions. A potential solution is to employ a hybrid systems approach, where safety is guaranteed in each individual mode within the system. This offsets complexity and responsibility from the individual controllers onto the complexity of determining discrete mode transitions. In this work we propose an efficient framework based on recursive neural networks and coreset data summarization to learn the transitions between an arbitrary number of controller modes that can have arbitrary complexity. Our approach allows us to efficiently gather annotation data from the large-scale datasets that are required to train such hybrid nonlinear systems to be safe under all operating conditions, favoring underexplored parts of the data. We demonstrate the construction of the embedding, and efficient detection of switching points for autonomous and nonautonomous car data. We further show how our approach enables efficient sampling of training data, to further improve either our embedding or the controllers</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ rosman2017hybrid, <br />
      author = {  
                  
                      Rosman, Guy and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Rus, Daniela 
                    
                   }, <br />
      title = { Hybrid control and learning with coresets for autonomous vehicles }, <br />
      
      
        booktitle = { IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) }, <br />
      
      
        year = { 2017 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<div class="card border-left-primary shadow mb-1">
          <div class="card-body">
            <div class="h2 font-weight-bold text-primary mb-1">2016</div>
          </div>
  </div>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="paull2016unified">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/paull2016unified.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>A Unified Resource-Constrained Framework for Graph SLAM</b></span>
    <span class="author">
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              Guoquan Huang,
            
          
        
      
      	
        
          and
          
            
              John J Leonard
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2016
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/http://liampaull.ca/publications/Paull_ICRA_2016.pdf" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
    <a href="https://montrealrobotics.github.io/assets/pdf/http://liampaull.ca/publications/Paull_ICRA_2016_poster.pptx" target="_blank" class="buttonMM">Poster</a>
  
  
    <a href="https://montrealrobotics.github.io/assets/pdf/http://liampaull.ca/publications/Paull_ICRA_2016_presentation.pptx" target="_blank" class="buttonMM">Slides</a>
  
  
  
    <a href="https://github.com/liampaull/Resource_Constrained" target="_blank" class="buttonPP">Code</a>
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Graphical methods have proven an extremely useful tool employed by the mobile robotics community to frame estimation problems. Incremental solvers are able to process incoming sensor data and produce maximum a posteriori (MAP) estimates in realtime by exploiting the natural sparsity within the graph for reasonable-sized problems. However, to enable truly longterm operation in prior unknown environments requires algorithms whose computation, memory, and bandwidth (in the case of distributed systems) requirements scale constantly with time and environment size. Some recent approaches have addressed this problem through a two-step process - first the variables selected for removal are marginalized which induces density, and then the result is sparsified to maintain computational efficiency. Previous literature generally addresses only one of these two components. In this work, we attempt to explicitly connect all of the aforementioned resource constraint requirements by considering the node removal and sparsification pipeline in its entirety. We formulate the node selection problem as a minimization problem over the penalty to be paid in the resulting sparsification. As a result, we produce node subset selection strategies that are optimal in terms of minimizing the impact, in terms of Kullback-Liebler divergence (KLD), of approximating the dense distribution by a sparse one. We then show that one instantiation of this problem yields a computationally tractable formulation. Finally, we evaluate the method on standard datasets and show that the KLD is minimized as compared to other commonly-used heuristic node selection techniques.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ paull2016unified, <br />
      author = {  
                  
                      Paull, Liam and
                    
                   
                  
                      Huang, Guoquan and
                    
                   
                  
                      Leonard, John J 
                    
                   }, <br />
      title = { A Unified Resource-Constrained Framework for Graph SLAM }, <br />
      
      
        booktitle = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2016 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="mu2016iros">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="https://montrealrobotics.github.io/assets/img/papers/mu2016iros.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Slam with objects using a nonparametric pose graph</b></span>
    <span class="author">
      
      	
        
          
            
              Beipeng Mu,
            
          
        
      
      	
        
          
            
              Shih-Yuan Liu,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              John Leonard,
            
          
        
      
      	
        
          and
          
            
              Jonathan P How
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE/RSJ International Conference onnIntelligent Robots and Systems (IROS)</em>
    
    
      2016
    
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1704.05959" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  
    <a href="https://www.youtube.com/watch?v=gOwMiFlj8KU" target="_blank" class="buttonSS">Video</a>
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Mapping and self-localization in unknown environments are fundamental capabilities in many robotic applications. These tasks typically involve the identification of objects as unique features or landmarks, which requires the objects both to be detected and then assigned a unique identifier that can be maintained when viewed from different perspectives and in different images. The data association and simultaneous localization and mapping (SLAM) problems are, individually, well-studied in the literature. But these two problems are inherently tightly coupled, and that has not been well-addressed. Without accurate SLAM, possible data associations are combinatorial and become intractable easily. Without accurate data association, the error of SLAM algorithms diverge easily. This paper proposes a novel nonparametric pose graph that models data association and SLAM in a single framework. An algorithm is further introduced to alternate between inferring data association and performing SLAM. Experimental results show that our approach has the new capability of associating object detections and localizing objects at the same time, leading to significantly better performance on both the data association and SLAM problems than achieved by considering only one and ignoring imperfections in the other.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @inproceedings{ mu2016iros, <br />
      author = {  
                  
                      Mu, Beipeng and
                    
                   
                  
                      Liu, Shih-Yuan and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Leonard, John and
                    
                   
                  
                      How, Jonathan P 
                    
                   }, <br />
      title = { Slam with objects using a nonparametric pose graph }, <br />
      
      
        booktitle = { IEEE/RSJ International Conference onnIntelligent Robots and Systems (IROS) }, <br />
      
      
        year = { 2016 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>


            <div class="my-5 pt-5 text-muted text-center text-md">
                <h4>&copy; Robotics and Embodied AI Lab Team, 2019</h4>
                <p class="affiliations">
                        <a href="https://en.diro.umontreal.ca/home/"><img src="https://montrealrobotics.github.io/assets/img/diro.png" width="150px" alt=
                            "Department of Computer Science and Operations Research"> </a>
                        |
                        <a href="https://www.umontreal.ca/">
                            <img src="https://montrealrobotics.github.io/assets/img/udem.png" width="150px" alt=
                            "Universit&eacute; de Montr&eacute;al">
                            </a>
                </p>
                <h4>Powered by jekyll</h4>
            </div>

        </div> <!-- /container -->

        <!-- Load jquery-->
        <script src="//code.jquery.com/jquery-1.12.4.min.js"></script>
        <!-- Support retina images. -->
        <script type="text/javascript"
                src="/assets/js/srcset-polyfill.js"></script>
        <!-- Support hidden abstract/bibtex blocks. -->
        <script src="/assets/js/common.js"></script>
    </body>
</html>
